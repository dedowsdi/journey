 vim: tw=80 sw=2 ts=2 et

-----------
2018.02.12:

math:
  inertial space:
    rotation only coordinate transform. 
    a_p1 = R * a;
    a_p2 = T * a_p1;

  sometimes it's necessary to do translate before rotation:
    a_p1 = T * a;   // can i call p1 space initrial_t space ?
    a_p2 = T * a_p1;

  create model view matrix with negative eye translation in world space:
    it's trivial to create model view by translation * rotation:
      mv_mat = T(w_pos) * R(w_rot) * T(-w_center)
    if your world rotation changed, but you want to reserve your eye position:
      // it should be understood as intrinsic transform.
      mv_mat_i = T(eye) * R(c_rot)
      mv_mat = R(w_rot) * T(-eye)

  rotate around y+:
    | c  0 s |
    | 0  1 0 |
    | -s 0 c |

  post translate:
    | A00   a01 | * | I t01 | = | .. A00*t01+a01    | =  A +   | 0 A00*t01   |
    | a10_t α11 |   | 0 1   |   | .. a10_t*t01+α11  |          | 0 a10_t*t01 |

    |  A00  | * |t01| =  a0 * τ0 + a1 * τ1 + a2 * τ2
    | a10_t |   |1  |
    
    ∴  A * T  =  | a0 a1 a2 a3+a0*τ0+a1*τ1+a2*τ2 |

  pre translate
    | I t01 | * | A00   a01 | = | A00+t01*a10_t a01+t01*α11 | 
    | 0 1   | * | a10_t α11 |   | ..                        | 
                              = A + | t01*a10_t t01*α11 | = A + | τ0*α30 τ0*α31 τ0*α32 τ0*α11|
                                    | 0         0       |       | τ1*α30 τ1*α31 τ1*α32 τ1*α11|
                                                                | τ2*α30 τ2*α31 τ2*α32 τ2*α11|
                                                                | 0      0      0      0     |

-----------
2018.02.16:

linear math:
  dot distribution:
    x_t·(y+z) = x_t·y + x_t·z
    (y+z)_t·x = y_t·x + z_t·x
    (x+y)_t·(x+y) = x_t·x + 2x_t·y + y_t·y

  laff matrix vector scalar greek alphabet:
    Matrix | vector | scalor | code    | note
    A      | a      | α      | alpha   | note
    B      | b      | β      | beta    |
    C      | c      | γ      | gamma   |
    D      | d      | δ      | delta   |
    E      | e      | ε      | epsilon | ei is the ith unit basis vector
    F      | f      | φ      | phi     |
    G      | g      | ξ      | xi      |
    H      | h      | ν      | eta     |
    I      |        |        |         | identity matrix
    K      | k      | κ      | kappa   |
    L      | l      | λ      | lambda  |
    M      | m      | μ      | mu      | m(·) = row dimension
    N      | n      | ν      | nu      | ν is shared with V. n(·) = column dimension
    O      |        |        |         | not used
    P      | p      | π      | pi      |
    Q      | q      | θ      | theta   |
    R      | r      | ρ      | rho     |
    S      | s      | σ      | sigma   |
    T      | t      | τ      | tao     |
    U      | u      | υ      | upsilon |
    V      | v      | ν      | nu      | ν is shared with N
    W      | w      | ω      | omega   |
    X      | x      | χ      | chi     |
    Y      | y      | ψ      | psi     |
    Z      | z      | ζ      | zeta    |

  if you partition matrix by rows, use ã_t(transpose a with hat) to represent it.

  given x∈ R_n, 
    x = Σ(i, 0, n-1, xi*ei)

  let L:R_n --> R_m be a linear transformation, given x∈ R_n, the result of y =
  L(x) is a vector in R_m, but then:
    y = L(x) = L(Σ(i, 0, n-1, χi*ei)) 
             = Σ(i, 0, n-1, L(χi*ei)) 
             = Σ(i, 0, n-1, χi*L(ei))
             = Σ(i, 0, n-1, χi*ai)  // let L(ei) = ai
             = Ax   // let A = | a0 a1 a2 .. am_1 |  //m_1 is m-1
    ∴ so any L(x) can be represented as A(x), ai = L(ei)
  
  ∵ A(αx) = αAx, A(x+y) = Ax+Ay
  ∴ A is linear

  ∴ A function f : R_n → R_m is a linear transformation iff it can be written as
    a matrix-vector multiplication.

linear math:
  matrix-matrix multiplication distributive:
    A(B+C) = AB + AC
    (B+C)A = BA + CA

  let A∈ R_mxn, let D denote a diagonal matrix:
    AD = |a0 a1 a2 ... an_1| D
       = |δ0*a0 δ1*a1 δ2*a2 ... δn_1*an_1|

    (DA) = | δ0*a0_h_t     |
           | δ1*a1_h_t     |
           |  ...          |
           | δn_1*an_1_h_t |

  if A, B ∈ R_n_n, and are both upper triangular matrix, then AB is also upper
  triangular matrix:
    C=AB
    assume i > j
      γij = ã_i_t·bj
          = Σ(k, 0, n-1, α_ik*β_kj )
        ∵ A is upper triangular, 
        ∴ α_ik = 0 if k < i
        ∵ B is upper triangular, 
        ∴ β_kj = 0 if k > j
        ∴ if k >=i ==> k > j ==> β_kj = 0
        ∴ γij = 0

  if A, B ∈ R_n_n, and are both lower triangular matrix, then AB is also lower
  triangular matrix:

  if B = AA_t B is symmetric
    βij = ã_i_t·ã_j = ã_j_t·ã_i = βji
  if B = A_tA B is symmetric
    βij = a_i_t·aj = a_j_t·ai = βji

  rank1 update:
    A = A + αxx_t

-----------
2018.02.21:

math:
  gaussian transform. L_j
    I 0        0 0 ... 0
    0 1        0 0 ... 0   // the jth row
    0 -λ_j+1,j 1 0 ... 0
    0 -λ_j+2,j 0 1 ... 0
    0 .        . . .   0
    0 .        . .  .  0
    0 .        . .   . 0
    0 -λ_m-1,j 0 0 ... 1

  L_j*A equals A except for i > j, (L_j*A)_h_i_t = ã_i_t - ã_j_t * λ_i,j

  use LU factorization to solve Ax = b :
    L is unit lower trianglular matrix, U is upper trianglular matrix.
    A = LU, U = L_inv*A, L_inv will the the final gaussian transform matrix

    forward substitution:
      solve Lz = b.  (z = apply gaussian transform to the right hand side)
      z = L_inv * b (the algorithm is nearly the same as LU factoriazation,
                    except b is an vector)

    backward substitution:
      solve Ux = z

-----------
2018.02.22:

math:
  If Gaussian elimination with row exchanges (LU factorization with pivoting)
  completes with an upper triangular system that has no zero diagonal
  coefficients, then for all right-hand side vectors, b, the linear system Ax =
  b has a unique solution, x.

  permutation vector:
    k_j ∈ {0,1..,n-1}, for 0<=j<=n-1.   k_i=k_j ==> i = j
    p = (k_0, k_1, ...k_n-1)_t 

  permutation matrix:
    P = P(p) = | e_k_0_t   |
               | e_k_1_t   |
                    .
                    .
                    .
               | e_k_n-1_t |

    P is the identity matrix with its rows(or cols) rearranged as indicated by
    the permutation vector (k 0 , k 1 , . . . , k n−1 ).

  pivot matrix:
    P(j) is a pivot matrix, it's result of swap 0 and jth row of I.

  if in ith step of gaussian elimination, the ith item of current row is 0, you
  can not do gaussian elimination, you must swap row, that's what pivot matrix
  used for:
    PA = LU
    PAx = Pb
    LUx = Pb
    Lz = Pb
    Ux = z

  gaussian elimation will fail if no row can be found that puts a nonzero on the
  diagonal(except the last one, if the appended one is also zero).

  if A is invertible:
   • A is nonsingular.
   • A_inv exists.
   • AA_inv = A_inv*A = I.
   • A represents a linear transformation that is a bijection.
   • Ax = b has a unique solution for all b ∈ R_n .
   • Ax = 0 implies that x = 0.
   • Ax = ej has a solution for all j ∈ {0, . . . , n − 1}.
   • The determinant of A is nonzero: det(A) != 0.
   • LU with partial pivot doesn't break down.
   • C(A) = R_n.  // ==> m=n ==> a0···an-1 is independent
   • A has linearly independent columns
   • N(A) = {0}.
   • rank(A) = n.

  inverse of special matrix:
    A = | 0 1 | A_inv = | 0 1 |
        | 1 0 |         | 1 0 |

    | α_0,0 α_0,1 |      | α_1,1 -α_0,1 |   
    | α_1,0 α_1,1 |_inv =| -α_1,0 α_0,0 |* (α_0,0*α_1,1 − α_1,0*α_0,1)

    | L_00   0    |        | L_00_inv               0      |
    | l_10_t λ_11 |_inv =  | -l_10_t*L_oo_inv/λ_11  1/λ_11 |
    so if lower triangular matrix has no 0 on it's diagnal, it's invertible.

    | U_00   u_01 |        | U_00_inv  -U_00_inv*u_01/υ_11 |
    | 0      υ11  |_inv =  | 0         1/υ_11              |
    so if upper triangular matrix has no 0 on it's diagnal, it's invertible.

    | I 0    0 |        | I  0    0 |
    | 0 1    0 |        | 0  1    0 |
    | 0 l_21 0 |_inv =  | 0 -l_21 0 |

  A_t_i = A_i_t
    AA_i = I
    A_i_t * A_t = I
    A_t_i = A_i_t

-----------
2018.02.23:

math:
  gauss-jordan elimination:
    AX = B
    transform A to I

  Matrix A is said to be symmetric positive definite (SPD) if
    • A is symmetric; and
    • x_T*Ax > 0 for all nonzero vectors x ∈ R_n.

  Cholesky factorization theorem:
    Let A ∈ R_n×n be a symmetric positive definite matrix. Then there exists a
    lower triangular matrix L ∈ R_n×n such that A = LL_T . If the diagonal
    elements of L are chosen to be positive, this factorization is unique.

-----------
2018.02.24:

math:

  row echelon form :  
                        | α00 α01 α02 ··· | 
                        | 0   α11 α12 ··· | 
                        | 0   0   α22 ··· | 
                        | 0   0   0   ··· | 

  reduced-echelon form :
                         | 1 0 0 ··· |
                         | 0 1 0 ··· |
                         | 0 0 1 ··· |
                         | 0 0 0 ··· |

  pivot : number in diagonal

  number of solutions of Ax=b:
    use gauss (jordan) elimination to transform |A b|:
      if there exists 0 = β != 0,  it has no solution
      if no pivot is 0, it has one solution.
      if some pivots is 0, it has infinite solution.

    to solve Ax=b with infinite solution:
      use gauss (jordan) elimination to transform |A b|
      find a specific solution x_s by settiong free variable to 0
      find a non-trivial solution x_n to Ax=0 by setting free variable vector to
      unit base vector one by one
      the general solution will be x_s + β0x_n0 + β1x_n1 + ···
      
  a vector space is a subset, S, of R_n with the following properties:
    • 0 ∈ S (the zero vector of size n is in the set S)
    • If v, w ∈ S then (v + w) ∈ S
    • If α ∈ R and v ∈ S then αv ∈ S
  S is closed under addition and scalar multiplication(linear).

  vector space of R_n is also called subsapce of R_n

  Let A ∈ R_m×n . Then the column space of A equals the set
  { Ax | x ∈ R_n } . It is denoted by C(A), it's a subspace of R_m

  Let A ∈ R_m×n . Then the row space of A equals the set
  { A_tx | x∈ R_m } . It is denoted by R(A), it's a subspace of R_n

  Let A ∈ R_m×n , x ∈ R_n , and b ∈ R_m. Then Ax = b has a solution if and only if b ∈ C(A).

  Let A ∈ R_m×n . Then the set of all vectors x ∈ R_n that have the property
  that Ax = 0 is called the null space of A and is denoted by N(A) = {x|Ax = 0}.

  Let {v0 , v1 , · · · , vn−1 } ⊂  R_m . Then the span of these vectors, Span{v 0
  , v1 , · · · , vn−1 }, is said to be the set of all vectors that are a linear
  combination of the given set of vectors, which is a subspace of R_m.

  If V = |v0 v1 ··· vn−1|, then Span(v0,v1 , . . . , vn−1 ) = C(V).

  A spanning set of a subspace S is a set of vectors {v0 , v1 , . . . , vn−1 }
  such that Span({v0 , v1 , . . . , vn−1 }) = S.

  Let {v0 , . . . , vn−1 } ⊂ R_m. Then this set of vectors is said to be
  linearly independent if χ0v0 + χ1v1 + · · · + χn−1vn−1 = 0 implies that χ0
  = · · · = χn−1 = 0. A set of vectors that is not linearly independent is said
  to be linearly dependent.

  if a set of vector is said to be linearly independent, none of these vector
  can be written as linear combination of the other vectors.

  Let {a0 , . . . , an−1 } ⊂ R_m and let A = | a0 ··· an−1 |. Then the vectors
  {a0 , . . . , an−1 } are linearly independent if and only if N(A) = {0}.

  Let {a0 , a1 , . . . , an−1 } ∈ R_m and n > m. Then these vectors are linearly
  dependent.

  Let S be a subspace of R_m . Then the set {v0 , v1 , · · · , vn−1 } ⊂ R_m is
  said to be a basis for S if:
    {v0 , v1 , · · · , vn−1 } are linearly independent 
    Span{v0 , v1 , · · · , vn−1 } = S.

  Let S ⊂ R_m . Then S contains at most m linearly independent vectors.

  Let S be a nontrivial subspace of R m . (In other words, S != {0}.) Then
  there exists a basis {v0 , v1 , . . . , vn−1 } ⊂ R_m such that Span(v0 , v1 ,
  . . . , vn−1 ) = S.

  Let S be a subspace of R_m and let {v0 , v1 , · · · , vn−1 } ⊂ R_m and {w0 ,
  w1 , · · · , wk−1 } ⊂ R_m both be bases for S.  Then k = n. In other words,
  the number of vectors in a basis is unique.

  The dimension of a subspace S equals the number of vectors in a basis for that
  subspace.

  Let A ∈ R_m×n . The rank of A equals the number of vectors in a basis for the
  column space of A. We will let rank(A) denote that rank.

  Let {v0 , v1 , · · · , vn−1 } ⊂ R_m be a spanning set for subspace S and
  assume that vi equals a linear combination of the other vectors. Then {v0 , v1
  , · · · , vi−1 , vi+1 , · · · , vn−1 } is a spanning set of S.

  Let {v0 , v1 , · · · , vn−1 } ⊂ R_m be linearly independent and assume that
  {v0 , v1 , · · · , vn−1 } ⊂ S where S is a subspace. Then this set of vectors
  is either a spanning set for S or there exists w ∈ S such that {v0 , v1 , · ·
  · , vn−1 , w} are linearly independent.

-----------
2018.02.25:

math:

1inch = 2.54cm
1foot = 12 inch = 30.48cm
9.68m = 31.75853feet ≈ 32feet
1yard = 3feet = 91.44cm
1mile = 5280feet = 1609.344m
1 league = 4,828.0417 meter (distance a man can cover in 1 hour)
1pound = 0.45359237kg

-----------
2018.02.26:

math:

任一排列中， 当某两个元素的先后次序与标准次序不同时， 就说有1个逆序.
一个排列中所有逆序的总数叫这个排列的逆序数。
逆序数为奇数的排列叫做奇排列，偶数的叫做偶排列。

det(A) = Σ(-1)^t(α_0,p0*α_1,p1*···α_n-1,p_n-1) 
det(A) = Σ(-1)^t(α_p0,0*α_p1,1*···α_pn-1,n-1) 
(there are n! different permutations, t is number of inversions)

if D is a diagonal matrix:
  det(D) = δ_0,0 * δ_1,1 * ··· δ_n-1,n-1

if D is reverse diagonal matrix
  det(D) =(-1)^(n*(n-1)/2)*δ_0,0 * δ_1,1 * ··· δ_n-1,n-1

if U is upper triangular matrix:
  det(U) = υ_0,0 * υ_1,1 * ··· υ_n-1,n-1

对换: 在排列中， 将任意两个元素对掉， 其余元素不动
相邻对换: 将相邻两元素对换。

1个排列中的任意两个元素对换，排列改变奇偶性。

det(A) = det(A_t)
det(A) = -1 * det(swap i, j row(col) of A)
det(A) = 0 if a_i = k*a_j, i != j
if B = A, b_i = k*a_i, det(B) = k*det(A)
if a_i = b_i + c_i
det(A) = det(|a0 a1 ... b_i ... an-1|) + det(|a0 a1 ... c_i ... an-1|)

A = | A_00   0    |,  det(A) = det(A_00) * det(A_11)
    | A_10   A_11 |

minor m_ij = det(delete ith row, jth col of A)
cofactor c_ij = (-1)^(i+j)(m_ij)

det(A) = a_i,0*det(c_i,0) + a_i,1*det(c_i,1) + ...a_i,n-1*det(c_i,n-1)

vandermonde:
  A = 
      | 1         1         ... 1           |
      | x_0       x_1       ... x_n-1       |
      | x_0^2     x_1^2     ... x_n-1^2     |
      | ...       ...       ... ...         |
      | ...       ...       ... ...         |
      | x_0^(n-1) x_1^(n-1) ... x_n-1^(n-1) |

  det(A) = Π_n>i>j>=0(x_i - x_j)

a_i,0*det(A_c_j,0) + a_i,1*det(A_c_j,1) + ...a_i,n-1*det(A_c_j,n-1) = 0 if i != j

law of cramer:
  if det(A) != 0, Ax=b has unique solution, x_i = det( a_i = b)  / det(A)

if Ax = b has 0 or more than 1 solutions, det(A) = 0

-----------
2018.02.27:

math:
  det(λA) = λ^n*det(A)
  det(AB) = det(A) * det(B)

  let A be nxn square matrix, it's adjugate matrix is:
    A_a = | c_0,0   c_1,0   ...   C_n-1,0   |
        = | c_0,1   c_1,1   ...   C_n-1,1   |
        = | ...     ...     ...   ...       |
        = | ...     ...     ...   ...       |
        = | c_0,n-1 c_1,n-1 ...   C_n-1,n-1 |

    A*A_a = A_a*A = det(A) I
    A_i = A_a / det(A)

  let A = PLP_i
      A^2 = PL^2P_i
      A^n = PL^nP_i

  A_t*A = 0 iff A = 0
    
-----------
2018.02.28:

math:
  basic matrix row transform:
   E(i,j)   ã_t_i <-> ã_t_j, 
   E(i(k))  ã_t_i = k * ã_t_i
   E(ij(k)) ã_t_i += k * ã_t_j

   E(i,j)_i = E(i,j)
   E(i(k))_i = E(i(k))/k
   E(ij(k))_i = E(ij(-k))

  if A after finite basic trasform become B, A〜B
  A〜B ==> B〜A
  A〜B && B〜C ==> A〜C
  Any matrix A can be transformed into standard form via basic transform:  
    | I 0 |
    | 0 0 |, rank(A) = dimension of I

  A~r~B iff PA = B, P is nonsingular
  A_i exists iff A~r~I
  A~c~B iff AQ = B, Q is nonsingular
  A〜B iff PAQ = B, P,Q is non singular

  let A be a square matrix, A_i exists iff A = P1P2...., Pi is bais matrix

  let A be a mxn matrix, pick k rows and columns(k<=m && k<=n), the crossing k^2
  elements elements will form a k order determinent, 称作A的k阶子式.

  设在A中有一个不为0的r阶子式D， 且所有r+1阶子式(存在的话)全为0,
  那么D称为A的最高阶非0子式， r = rank(A) or R(A), 0矩阵的秩为0

  0<=R(A)<=min(m,n)
  R(A) = R(A^t)
  if A is n squre, R(A) = n iff det(A) != 0
  if A〜B , then R(A) = R(B)
  if P,Q is nonsingular, R(PAQ) = R(A)
  max(R(A), R(B)) <= R(A,B) <= R(A) + R(B)
  R(A+B)<=R(A)+R(B)
  R(AB)<=min(R(A), R(B))
  if let A be mxn, B be nxl AB=0 then R(A) + R(B) <=n

  Ax=b, (b∈ R_n) has 0 solusion if R(A) < R(A, b)
             1 solution if R(A) = R(A,b) = n
             n solution if R(A) = R(A,b) < n

  AX=B has solutions iff R(A) = R(A, B)

-----------
2018.03.01:

math:
  {x=(x0,x1....xn-1)_t | a0x0+a1x1+....an-1xn-1 = b }
  叫做n唯向量空间R_n中的n-1唯超平面

  if b1,b2,...bl are linear combination s of a1,a2,...am, R(|b1,b2...|) <=
  R(|a1,a2...|)

  如果 a0,a1...an-1 和 b0,b1,..bn-1 能相互线性表示， 则两个向量组等价

  a0,a1..an-1 is linearly dependent iff  R(|a0,a1..an-1|) < n; a0,a1...an-1 is linearly
  independent iff R(|a0,a1...an-1|) = n

  最大无关组所包含的向量个数称为向量组的秩

  R(A) = R(a0,a1....an-1) = R(a_h_0, a_h_1, .....a_h_n-1)

  设mxn矩阵A的秩R(A) = r, 则n元齐次线性方程组Ax=0的解集S的秩R(S) = n-r

  R(A_tA) = R(A)

-----------
2018.03.02:

math:
  Let V, W ⊂ R_n be subspaces. Then V and W are said to be orthogonal if and only
  if v ∈ V and w ∈ W implies that v_t*w = 0.

  Given subspace V ⊂ R_n , the set of all vectors in R_n that are orthogonal to
  V is denoted by V^⊥ (pronounced as “V-perp”).

  Let A ∈ R_m×n . Then R(A)⊥ N(A).

  Let A ∈ R_m×n . Then every x ∈ R_n can be written as x = x_r + x_n where x_r ∈
  R(A) and x_n ∈ N(A).

  ∵ Ax_r = Ax_r + 0 = A(x_r + x_n) = Ax
  ∴ We conclude that if Ax = b has a solution, then there is a x_r ∈ R(A) such
  that Ax_r = b.

  Let A ∈ R m×n . Then A is a one-to-one, onto mapping from R (A) to C ( A ).

  Given A ∈ R_m×n the left null space of A is the set of all vectors x such that
  x_t*A = 0, Clearly, the left null space of A equals the null space of A_T .

  Let A ∈ R_m×n . Then the left null space of A is orthogonal to the column
  space of A and the dimension of the left null space of A equals m − r, where r
  is the dimension of the column space of A.

  If A ∈ R_m×n(m need not equal to n) has linearly independent columns, then
  A_t*A is nonsingular (equivalently, has an inverse, A_t*A*x̂ = A_t*b has a
  solution for all b, etc.).

  Let A ∈ R_m×n, b ∈ R_m , and x ∈ R_n and assume that A has linearly
  independent columns. Then the solution that minimizes the length of the vector
  b − Ax is given by x̂ = (A_t*A)_i*A_t*b.

  if Ax=b has no solution, b !∈  C(A), assume z∈ C(A) and b = z + w, inorder to
  minimize ‖b-z‖, wo let w⊥ z, which means w∈ N(A_t) which means:
    A_t*w = 0
    A_t(b-z) = 0
    A_t*z = A_t*b
    A_t*A*x̂ = A_t*b
    x̂ = (A_t*A)_i*A_t*b

    z = Ax̂ = A*(A_t*A)_i*A_t*b  //proj of b on C(A)

    The orthogonal projection of b onto C(A)⊥ is given by 
      b − z = [I − A(A_t*A)_i*A_t]b.

  Let A ∈ R_m×n . If A has linearly independent columns, then A† = (A_t*A)_i*A_t
  is called the (left) pseudo inverse. Note that this means m ≥ n and A†*A =
  (A_t*A)_i*A_t*A = I.

-----------
2018.03.05:

math: 
  Let q0 , q1 , . . . , qk−1 ∈ R_m . Then these vectors are (mutually)
  orthonormal if for all 0 ≤ i, j < k :
    qi_t * qj = 1   if i == j
    qi_t * qj = 0   otherwise

  Let a0 , a1 , . . . , ak−1 ∈ R m be linearly independent vectors and let q0 ,
  q1 , . . . , qk−1 ∈ R m be the result of Gram- Schmidt orthogonalization.
  Then Span({a0 , a1 , . . . , ak−1 }) = Span({q0 , q1 , . . . , qk−1 }).

  A matrix Q ∈ R_m×n has mutually orthonormal columns if and only if Q_t*Q = I.
  A matrix Q ∈ R_m×n has mutually orthonormal rows if and only if Q*Q_t = I.

-----------
2018.03.06:

math:
  QR factorization:
    (a0 a1 ...an-1) = (q0 q1 ... qn-1) * | ρ_0,0   ρ_0,1   ... ρ_0,n-1   |
                                         | 0       ρ_1,1   ... ρ_1,n-1   |
                                         | 0       ...     ... ...       |
                                         | 0       0       ... ρ_n-1,n-1 |
    in k th column:
      for p = 0...k-1
        ρ_p,k = qp_t * ak
      ak_⊥  = ak - Σ_p_0_k-1(ρ_p,k * q_p)
      ρ_k,k = ‖ak_⊥ ‖  
      q_k = ak_⊥ /ρ_k,k

  to solve Ax ≈ b
    x = (A_t*A)_i*A_t *b
      = ((QR)_t * QR )_i * (QR)_t * b
      = (R_t*R)_i * R_t*Q_t*b
      = R_i*Q_t*b
    Rx = Q_t*b

-----------
2018.03.07:

math:
  conpoment of b in direction of a:
    u = a_t*b*a/(a_t*a)

----------
2018.03.08

math:
  if Ax = λx, scalars λ that satisfy it for nonzero vector x are known as
  eigenvalues while the nonzero vectors is knows as eigenvectors. (λ,x) is said
  to be an eigenpair.

  to solve this, rearrange it as (A-λI)x = 0, A must be square, (A-λI) must be
  singular. x∈ N(A-λI)

  the set of all vectors x that satisfy Ax = λx is a subspace. every vector in
  this subspace except 0 is an eigenvector.

  Eigenpairs for diagnal matrix are given by (δ0,e0), (δ1,e1), · · · , (δn−1, en−1)

  The eigenvalues of upper or lower triangular matrix are α0,0, α1,1,...αn-1,n-1

----------
2018.03.09

math:
  let A be nxn matrix, if X_iAX = Λ, the matrix X is said to diagnalize matrix
  A. rank(X) = n.

  Matrices for which such a matrix X does not exists are called defective
  matrix.

  Given A ∈ R_n×n ,
  p_n (λ) = det(A − λI) = λ^n + γ_n−1*λ^n−1 + · · · + γ_1*λ + γ_0 
  for some coefficients γ_0, γ_1 , . . . , γ_n−1 ∈ R.

  The set of all roots of p_n(λ), which is the set of all eigenvalues of A, is
  denoted by Λ(A) and is called the spectrum of matrix A.

  Given A ∈ R_n×n , p_n(λ) = det(A − λI) is called the characteristic
  polynomial.

  The characteristic polynomial can be factored as 
  p_n(λ) = det(A − λI) = (λ − λ 0 )^n_0*(λ − λ_1)^n_1 · · · (λ − λ_k−1 )^n_k−1 ,
  where n0 + n1 + · · · + nk−1 = n and nj is the root λ j , which is known
  as the (algebraic) multiplicity of eigenvalue λ_j .

----------
2018.03.10

math:
  jordan block has eigenvalue λ of algebraic multiplicity k, but geometric
  multiplicity one (it has only one linearly independent eigenvector).

    J_k(λ) =  | λ 1 0 ... 0 0 |
              | 0 λ 1 ... 0 0 |
              | 0 0 λ ... 0 0 |
              | ...       0 0 |
              | 0 0 0 ... λ 1 |
              | 0 0 0 ... 0 λ |
    Λ(J) = {λ}

  The geometric multiplicity of an eigenvalue λ equals the number of linearly
  independent eigenvectors that are associated with λ.

  Let A ∈ R_n×n and A_i,i are a square matrices. Then Λ(A) = Λ(A_0,0 ) ∪ Λ(A_1,1
  ) ∪ · · · ∪ Λ(A_N−1,N−1 ).

  Let A ∈ R_n×n be symmetric, λ_i != λ_j , Ax_i = λ_i*x_i and Ax_j = λ_j*x_j
  then xi_t*x_j = 0

  Let A ∈ R n×n be symmetric. Then its eigenvalues are real valued.

  Let Ax = λx and k ≥ 1. A^k*x = λ^k*x

  A ∈ R_n×n is nonsingular if and only if 0 !∈  Λ(A).

----------
2018.04.03

math:
  fahrenheit and celsius
  0C = 32F
  100C = 212F
  which means 1C change is equal to 1.8F change
  so to change from F to C:
    (x-32)/1.8
  to change from C to F
    x*1.8 + 32

----------
2019.01.22

cartesian product:
  a * b_t
